{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtFlorence=pd.read_csv(\"datasetsClean/florence_cleaned.csv\")\n",
    "#dtFlorence.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "dtFlorence.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIE FUNZIONI DI PULIZIA DEL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tolgo le colonne con caratteristiche non strutturali e quelle d'intralcio per la clusterizzazione\n",
    "def getDtCaratterisitcheStrutturali(dataset):\n",
    "       datasetStrutturale = dataset.drop(['Unnamed: 0', 'amenities', 'minimum_nights', 'maximum_nights',\n",
    "              'minimum_minimum_nights', 'maximum_minimum_nights',\n",
    "              'minimum_maximum_nights', 'maximum_maximum_nights',\n",
    "              'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'number_of_reviews',\n",
    "              'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "              'review_scores_rating', 'review_scores_accuracy',\n",
    "              'review_scores_cleanliness', 'review_scores_checkin',\n",
    "              'review_scores_communication', 'review_scores_location',\n",
    "              'review_scores_value', 'license', 'latitude', 'longitude', 'price'], axis=1)\n",
    "       return datasetStrutturale\n",
    "\n",
    "def oneHotEncoding_neighbourhood_cleansed(df):\n",
    "    # Applica l'encoding one-hot alla colonna neighbourhood_cleansed\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    neighbourhood_cleansed_encoded = encoder.fit_transform(df[['neighbourhood_cleansed']])\n",
    "\n",
    "    # Converte l'array risultante in un DataFrame\n",
    "    neighbourhood_cleansed_encoded_df = pd.DataFrame(neighbourhood_cleansed_encoded, columns=encoder.get_feature_names_out(['neighbourhood_cleansed']))\n",
    "\n",
    "    # Concatena il DataFrame codificato con il DataFrame originale, eliminando la colonna neighbourhood_cleansed\n",
    "    df_encoded = pd.concat([df.drop(columns=['neighbourhood_cleansed']), neighbourhood_cleansed_encoded_df], axis=1)\n",
    "    return df_encoded\n",
    "\n",
    "def createLabel(df):\n",
    "    # Funzione per etichettare le righe uguali\n",
    "    def label_rows(row):\n",
    "        return '_'.join([str(row[col]) for col in df.columns])\n",
    "    \n",
    "    # Creazione della nuova colonna 'label'\n",
    "    df['label'] = df.apply(label_rows, axis=1)\n",
    "\n",
    "    # Mapping degli indici univoci ai valori della colonna 'label'\n",
    "    label_map = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "\n",
    "    # Assegnazione dei valori della label\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "    df['id'] = df.reset_index().index\n",
    "    dtFlorence['id'] = dtFlorence.reset_index().index\n",
    "    return df\n",
    "\n",
    "def mergeColumns(df, origin_df):\n",
    "       add_columns = ['id', 'amenities', 'minimum_nights', 'maximum_nights',\n",
    "              'minimum_minimum_nights', 'maximum_minimum_nights',\n",
    "              'minimum_maximum_nights', 'maximum_maximum_nights',\n",
    "              'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'number_of_reviews',\n",
    "              'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "              'review_scores_rating', 'review_scores_accuracy',\n",
    "              'review_scores_cleanliness', 'review_scores_checkin',\n",
    "              'review_scores_communication', 'review_scores_location',\n",
    "              'review_scores_value', 'license', 'price']\n",
    "       # Supponendo che 'id' sia la colonna chiave comune tra i due DataFrame\n",
    "       df_merged = df.merge(origin_df[add_columns], on='id', how='left')\n",
    "       return df_merged\n",
    "\n",
    "def getCaseTempo0(df):\n",
    "    grouped = df.groupby('label')\n",
    "\n",
    "    # Creo una maschera booleana che seleziona tutte le righe con il valore minimo nella colonna 'number_of_reviews' per ciascun gruppo\n",
    "    min_reviews_mask = df['number_of_reviews'] == grouped['number_of_reviews'].transform('min')\n",
    "\n",
    "    rows_with_min_reviews = df[min_reviews_mask]\n",
    "\n",
    "    return rows_with_min_reviews\n",
    "\n",
    "#modifica per altri due dataset\n",
    "def createAnalysisDf(carattStrutt, dfMerged):\n",
    "    new_columns = []\n",
    "    for col in dfMerged:\n",
    "        if col in carattStrutt:\n",
    "            new_columns.append(col)\n",
    "        else:\n",
    "            col_0 = col + '_0'\n",
    "            col_1 = col + '_1'\n",
    "            new_columns.append(col_0)\n",
    "            new_columns.append(col_1)\n",
    "\n",
    "    analysisDf = pd.DataFrame(columns=new_columns)\n",
    "    return analysisDf\n",
    "\n",
    "def createArrayCarattStrutt(dfStrutturale):\n",
    "    # salvo in un array tutte le caratteristiche strutturali, poi tutte quelle non strutturali al tempo 0\n",
    "    carattStrutt = dfStrutturale.columns\n",
    "    carattStrutt = list(carattStrutt)\n",
    "    carattStrutt.remove('id')\n",
    "    return carattStrutt\n",
    "\n",
    "def createArrayCarattNonStrutt(df):\n",
    "    carattNonStrutt_0 = [col for col in df.columns if col.endswith('_0')]\n",
    "    # Creazione dell'array contenente i nomi delle colonne che terminano con _1\n",
    "    carattNonStrutt_1 = [col for col in df.columns if col.endswith('_1')]\n",
    "    return (carattNonStrutt_0, carattNonStrutt_1)\n",
    "\n",
    "def populateAnalysisDf(df_t0, df_labeled, carattStrutt, carattNonStrutt_0, carattNonStrutt_1, analysisDf):\n",
    "    rows_to_add = []\n",
    "    \n",
    "    for _, row in df_t0.iterrows():\n",
    "        label = row['label']\n",
    "        min_reviews = row['number_of_reviews']\n",
    "        id_value = row['id']\n",
    "        \n",
    "        filtered_rows = df_labeled.loc[df_labeled['label'] == label]\n",
    "        \n",
    "        for _, row_filtered in filtered_rows.iterrows():\n",
    "            if row_filtered['id'] != id_value and row_filtered['number_of_reviews'] != min_reviews:\n",
    "                new_row = {}\n",
    "                for column in analysisDf.columns:\n",
    "                    if column in carattStrutt:\n",
    "                        new_row[column] = row[column]\n",
    "                    elif column in carattNonStrutt_0:\n",
    "                        column_without_suffix = column[:-2]\n",
    "                        new_row[column] = row[column_without_suffix]\n",
    "                    elif column in carattNonStrutt_1:\n",
    "                        column_without_suffix = column[:-2]\n",
    "                        new_row[column] = row_filtered[column_without_suffix]\n",
    "                \n",
    "                rows_to_add.append(new_row)\n",
    "\n",
    "    analysisDf = analysisDf.append(rows_to_add, ignore_index=True)\n",
    "    return analysisDf\n",
    "\n",
    "# FUNZIONE DI POPOLAMENTO DATASET ALTERNATIVA CHE BILANCIA LE CASE AL T_0 E AL T_1 \n",
    "def populateAnalysisDf2(df_labeled):\n",
    "    listOfDt = []\n",
    "    for i in range(0,max(df_labeled['label']),1):\n",
    "        t_0=df_labeled.loc[(df_labeled['label']==i) & (df_labeled['number_of_reviews']==min(df_labeled.loc[df_labeled['label']==i]['number_of_reviews']))]\n",
    "        t_1=df_labeled.loc[(df_labeled['label']==i) & (df_labeled['number_of_reviews']!=min(df_labeled.loc[df_labeled['label']==i]['number_of_reviews']))]\n",
    "        \n",
    "        if(len(t_0)>len(t_1)):\n",
    "            nrRow = len(t_1)\n",
    "            t_0 = t_0.head(nrRow)\n",
    "        else:\n",
    "            nrRow = len(t_0)\n",
    "            t_1 = t_1.head(nrRow)\n",
    "\n",
    "        t_0['index'] = range(len(t_0))\n",
    "        t_1['index'] = range(len(t_1))\n",
    "        T_0_1 = pd.merge(t_0,t_1,how='outer',on='index')\n",
    "        listOfDt.append(T_0_1)\n",
    "    \n",
    "    return pd.concat(listOfDt,ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtFlorenceStrutturale = getDtCaratterisitcheStrutturali(dtFlorence)\n",
    "dtFlorenceStrutturale = oneHotEncoding_neighbourhood_cleansed(dtFlorenceStrutturale)\n",
    "caratteristicheStrutturali = dtFlorenceStrutturale.columns.to_list()\n",
    "dtFlorenceStrutturale = createLabel(dtFlorenceStrutturale)\n",
    "dtFlorenceLabeled = mergeColumns(dtFlorenceStrutturale, dtFlorence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_min_reviews = getCaseTempo0(dtFlorenceLabeled)\n",
    "carattStrutt = createArrayCarattStrutt(dtFlorenceStrutturale)\n",
    "dtFlorenceAnalysis = createAnalysisDf(carattStrutt, dtFlorenceLabeled)\n",
    "carattNonStrutt_0, carattNonStrutt_1 = createArrayCarattNonStrutt(dtFlorenceAnalysis)\n",
    "dtFlorenceAnalysis = populateAnalysisDf(rows_with_min_reviews, dtFlorenceLabeled, carattStrutt, carattNonStrutt_0, carattNonStrutt_1, dtFlorenceAnalysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtFlorenceAnalysis = populateAnalysisDf2(dtFlorenceLabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtFlorenceAnalysis.to_csv(\"./datasetsAnalysis/florence_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaValoriUnici(dataset,colonna):\n",
    "    for i in dataset[colonna].unique():\n",
    "        print(\"Valore \",i,\"counts: \",(dataset[colonna] ==i).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZZAZIONE DELLA DISTRIBUZIONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showHistogram(dataset, column_name):\n",
    "    maxValue = max(dataset[column_name])\n",
    "    minValue = min(dataset[column_name])\n",
    "    columnNames = dataset[column_name].unique()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(dataset[column_name],bins=300, color='skyblue', edgecolor='black')\n",
    "    plt.title('Histogram')\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    #plt.xticks(range(minValue,maxValue, len(columnNames)))\n",
    "    plt.gca().set_xticklabels(columnNames)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtFlorenceAnalysis = pd.read_csv(\"./datasetsAnalysis/florence_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtFlorenceAnalysis = dtFlorenceAnalysis.loc[dtFlorenceAnalysis[\"number_of_reviews_x\"]==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDelete = ['neighbourhood_group_cleansed',\"neighbourhood\", \"amenities_0\",\"amenities_1\",\"id_0\",\"id_1\",\"label\"]\n",
    "dtFlorenceAnalysis.drop(featureDelete,axis=1,inplace=True)\n",
    "dtFlorenceAnalysis.drop(dtFlorenceAnalysis.columns[0:13],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLO DI APPRENDIMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dtFlorenceAnalysis.iloc[:,0:37]\n",
    "y = dtFlorenceAnalysis[[\"price_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop([\"price_0\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train,X_test,y_train,y_test  = scaler.fit_transform(X_train),scaler.fit_transform(X_test),scaler.fit_transform(y_train),scaler.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regr =  LinearSVR(dual=True, random_state=42, tol=1e-2,loss='squared_epsilon_insensitive')\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOMFOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest \n",
    "regressor = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "# Train the \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT DEI RISULTATI OTTENUTI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloumnNumber = X.shape[1]\n",
    "columnNames = dtFlorenceAnalysis.iloc[:,0:coloumnNumber].columns.to_list()\n",
    "plt.figure(figsize=(15.75,11.81))\n",
    "plt.bar(range(0,coloumnNumber,1),regr.coef_)\n",
    "plt.xticks(range(0,coloumnNumber,1),columnNames)\n",
    "plt.gca().set_xticklabels(columnNames)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
